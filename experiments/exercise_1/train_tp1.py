"""
TP1 - Ejercicio 1: PerceptrÃ³n Simple para Funciones LÃ³gicas AND y XOR

ImplementaciÃ³n bÃ¡sica del algoritmo de perceptrÃ³n simple con funciÃ³n escalÃ³n bipolar.
Sin usar librerÃ­as de ML, solo NumPy para operaciones matemÃ¡ticas bÃ¡sicas.
"""

import os
import sys
import numpy as np
from pathlib import Path

# Agregar el directorio raÃ­z al path para imports
sys.path.append(str(Path(__file__).parent.parent.parent))

from neural_network.core.perceptron import Perceptron


def create_and_gate_data():
    """
    Crear datos para la funciÃ³n lÃ³gica AND.
    Entradas: {-1,1}, {1,-1}, {-1,-1}, {1,1}
    Salidas:  {-1}, {-1}, {-1}, {1}
    """
    X = np.array([
        [-1, 1],
        [1, -1], 
        [-1, -1],
        [1, 1]
    ])
    
    y = np.array([-1, -1, -1, 1])
    
    return X, y


def create_xor_gate_data():
    """
    Crear datos para la funciÃ³n lÃ³gica XOR.
    Entradas: {-1,1}, {1,-1}, {-1,-1}, {1,1}
    Salidas:  {1}, {1}, {-1}, {-1}
    """
    X = np.array([
        [-1, 1],
        [1, -1],
        [-1, -1], 
        [1, 1]
    ])
    
    y = np.array([1, 1, -1, -1])
    
    return X, y


def train_perceptron(problem_type: str, learning_rate: float = 0.1, max_epochs: int = 100, seed: int = 42):
    """
    Entrenar un perceptrÃ³n simple para el problema especificado.
    
    Args:
        problem_type: "and" o "xor"
        learning_rate: Tasa de aprendizaje
        max_epochs: NÃºmero mÃ¡ximo de Ã©pocas
        seed: Semilla para reproducibilidad
    """
    print(f"\n{'='*50}")
    print(f"ENTRENANDO PERCEPTRÃ“N PARA {problem_type.upper()}")
    print(f"{'='*50}")
    
    # Configurar semilla
    if seed is not None:
        np.random.seed(seed)
    
    # Crear datos segÃºn el problema
    if problem_type == "and":
        X, y = create_and_gate_data()
        print("Problema: FunciÃ³n lÃ³gica AND")
    elif problem_type == "xor":
        X, y = create_xor_gate_data()
        print("Problema: FunciÃ³n lÃ³gica XOR")
    else:
        raise ValueError("problem_type debe ser 'and' o 'xor'")
    
    print(f"\nDatos de entrenamiento:")
    print("Entrada  | Salida")
    print("-" * 17)
    for i, (inputs, target) in enumerate(zip(X, y)):
        print(f"[{inputs[0]:2}, {inputs[1]:2}] |   {target:2}")
    
    # Crear perceptrÃ³n usando la clase existente
    perceptron = Perceptron(num_inputs=2, activation_type="STEP_BIPOLAR")
    
    print(f"\nParÃ¡metros:")
    print(f"  - Learning rate: {learning_rate}")
    print(f"  - Max epochs: {max_epochs}")
    print(f"  - Seed: {seed}")
    print(f"  - Pesos iniciales: [{perceptron.weights[0]:.4f}, {perceptron.weights[1]:.4f}]")
    print(f"  - Bias inicial: {perceptron.bias:.4f}")
    
    # Algoritmo de entrenamiento clÃ¡sico del perceptrÃ³n
    print(f"\n--- INICIANDO ENTRENAMIENTO ---")
    converged = False
    
    for epoch in range(max_epochs):
        total_errors = 0
        epoch_updates = 0
        
        # Entrenar con cada muestra individualmente
        for i, (inputs, target) in enumerate(zip(X, y)):
            # Preparar entrada como matriz (1 sample)
            x_input = inputs.reshape(1, -1)
            
            # Forward pass
            prediction = perceptron.calculate_output(x_input)
            predicted_class = 1 if prediction[0] >= 0 else -1
            
            # Calcular error
            error = target - predicted_class
            
            if error != 0:
                total_errors += abs(error)
                epoch_updates += 1
                
                # ActualizaciÃ³n manual de pesos (algoritmo clÃ¡sico de perceptrÃ³n)
                # w = w + lr * error * x
                perceptron.weights += learning_rate * error * inputs
                
                # b = b + lr * error
                perceptron.bias += learning_rate * error
                
                print(f"  Ã‰poca {epoch+1:2d}, Muestra {i+1}: Error={error:2}, "
                      f"Pesos=[{perceptron.weights[0]:6.3f}, {perceptron.weights[1]:6.3f}], "
                      f"Bias={perceptron.bias:6.3f}")
        
        # Calcular accuracy de la Ã©poca
        predictions = []
        for inputs in X:
            x_input = inputs.reshape(1, -1)
            pred = perceptron.calculate_output(x_input)
            pred_class = 1 if pred[0] >= 0 else -1
            predictions.append(pred_class)
        
        accuracy = np.mean(np.array(predictions) == y)
        print(f"Ã‰poca {epoch+1:2d}: Errores={total_errors}, Accuracy={accuracy:.3f}, Updates={epoch_updates}")
        
        # Verificar convergencia
        if total_errors == 0:
            converged = True
            print(f"\nÂ¡Convergencia alcanzada en Ã©poca {epoch + 1}!")
            break
    
    if not converged:
        print(f"\nNo convergiÃ³ en {max_epochs} Ã©pocas")
    
    # EvaluaciÃ³n final
    print(f"\n--- EVALUACIÃ“N FINAL ---")
    print("Entrada   | Esperado | Predicho | Â¿Correcto?")
    print("-" * 40)
    
    final_predictions = []
    for i, (inputs, target) in enumerate(zip(X, y)):
        x_input = inputs.reshape(1, -1)
        pred = perceptron.calculate_output(x_input)
        pred_class = 1 if pred[0] >= 0 else -1
        final_predictions.append(pred_class)
        
        status = "âœ“" if target == pred_class else "âœ—"
        print(f"[{inputs[0]:2}, {inputs[1]:2}] |    {target:2}    |    {pred_class:2}    |     {status}")
    
    final_accuracy = np.mean(np.array(final_predictions) == y)
    print(f"\nAccuracy: {final_accuracy:.3f} ({final_accuracy*100:.1f}%)")
    print(f"Pesos finales: [{perceptron.weights[0]:.4f}, {perceptron.weights[1]:.4f}]")
    print(f"Bias final: {perceptron.bias:.4f}")
    
    # Mostrar frontera de decisiÃ³n
    if abs(perceptron.weights[1]) < 1e-10:
        print(f"\nFrontera de decisiÃ³n:")
        print(f"  LÃ­nea vertical: x1 = {-perceptron.bias / perceptron.weights[0]:.4f}")
    else:
        slope = -perceptron.weights[0] / perceptron.weights[1]
        intercept = -perceptron.bias / perceptron.weights[1]
        print(f"\nFrontera de decisiÃ³n:")
        print(f"  EcuaciÃ³n: x2 = {slope:.4f} * x1 + {intercept:.4f}")
    
    return final_accuracy, converged, perceptron


def analyze_perceptron_limitations():
    """
    Analizar las limitaciones del perceptrÃ³n simple basado en los resultados.
    """
    print("\n" + "="*60)
    print("TP1 - ANÃLISIS DE LIMITACIONES DEL PERCEPTRÃ“N SIMPLE")
    print("="*60)
    
    # Entrenar ambos problemas
    and_accuracy, and_converged, and_perceptron = train_perceptron("and", max_epochs=50)
    xor_accuracy, xor_converged, xor_perceptron = train_perceptron("xor", max_epochs=50)
    
    print(f"\n{'='*60}")
    print("RESUMEN COMPARATIVO")
    print("="*60)
    
    print(f"\nðŸ”¹ Problema AND:")
    print(f"  - Accuracy final: {and_accuracy:.3f} ({and_accuracy*100:.1f}%)")
    print(f"  - Â¿Converge?: {'SÃ' if and_converged else 'NO'}")
    
    print(f"\nðŸ”¹ Problema XOR:")
    print(f"  - Accuracy final: {xor_accuracy:.3f} ({xor_accuracy*100:.1f}%)")
    print(f"  - Â¿Converge?: {'SÃ' if xor_converged else 'NO'}")
    
    print(f"\n{'='*60}")
    print("CONCLUSIONES")
    print("="*60)
    
    print("\nðŸ” 1. SEPARABILIDAD LINEAL:")
    print("   â€¢ El problema AND ES linealmente separable")
    print("   â€¢ El problema XOR NO ES linealmente separable")
    
    print("\nðŸš« 2. LIMITACIONES DEL PERCEPTRÃ“N SIMPLE:")
    print("   â€¢ Solo puede resolver problemas linealmente separables")
    print("   â€¢ Utiliza una lÃ­nea recta como frontera de decisiÃ³n")
    print("   â€¢ XOR requiere una frontera de decisiÃ³n no lineal")
    
    print("\nðŸ§  3. CAPACIDAD REPRESENTACIONAL:")
    print("   â€¢ Un perceptrÃ³n simple solo puede aprender funciones lineales")
    print("   â€¢ Para problemas como XOR se necesitan redes multicapa (MLP)")
    
    print("\nðŸ“ˆ 4. CONVERGENCIA:")
    if and_converged:
        print("   â€¢ AND converge porque es linealmente separable")
    if not xor_converged:
        print("   â€¢ XOR no converge porque no es linealmente separable")
    
    print(f"\n{'='*60}")
    print("Â¿QUÃ‰ PROBLEMAS PUEDE RESOLVER EL PERCEPTRÃ“N SIMPLE?")
    print("="*60)
    print("âœ… Funciones lÃ³gicas: AND, OR, NOT")
    print("âœ… ClasificaciÃ³n binaria linealmente separable")
    print("âœ… RegresiÃ³n lineal simple")
    print("âŒ Funciones lÃ³gicas: XOR, XNOR")
    print("âŒ Problemas no linealmente separables")
    print("âŒ ClasificaciÃ³n multiclase compleja")


if __name__ == "__main__":
    print("TP1 - EJERCICIO 1: PERCEPTRÃ“N SIMPLE")
    print("ImplementaciÃ³n sin librerÃ­as de ML (solo NumPy)")
    print("-" * 50)
    
    # Ejecutar anÃ¡lisis completo
    analyze_perceptron_limitations()